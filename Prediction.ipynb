{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this seminar assignment, we will explore the data and build machine-learning models that predict the biodegradability of chemicals.\n",
    "\n",
    "[Data set](https://www.openml.org/search?type=data&status=active&id=1494&sort=runs) containing values for \n",
    "**41 attributes** (`molecular descriptors`) used to classify 1055 chemicals into **2 classes** (`ready` and `not ready` biodegradable).\n",
    "\n",
    "## Attribute information\n",
    "\n",
    "41 molecular descriptors (features) and 1 experimental class:\n",
    "\n",
    "| Feature Name | Feature Information (Molecular Descriptor) | Type | Distinct values / Missing attributes |\n",
    "| --- | --- | --- | --- |\n",
    "| Class (target) | ready biodegradable (RB) and not ready biodegradable (NRB) | nominal | 2/0 |\n",
    "| V1  | SpMax_L: Leading eigenvalue from Laplace matrix | numeric | 440/0 |\n",
    "| V2  | J_Dz(e): Balaban-like index from Barysz matrix weighted by Sanderson electronegativity | numeric | 1022/0 |\n",
    "| V3  | nHM: Number of heavy atoms | numeric | 11/0 |\n",
    "| V4  | F01\\[N-N\\]: Frequency of N-N at topological distance 1 | numeric | 4/0 |\n",
    "| V5  | F04\\[C-N\\]: Frequency of C-N at topological distance 4 | numeric | 16/0 |\n",
    "| V6  | NssssC: Number of atoms of type ssssC | numeric | 13/0 |\n",
    "| V7  | nCb-: Number of substituted benzene C(sp2) | numeric | 15/0 |\n",
    "| V8  | C%: Percentage of C atoms | numeric | 188/0 |\n",
    "| V9  | nCp: Number of terminal primary C(sp3) | numeric | 15/0 |\n",
    "| V10 | nO: Number of oxygen atoms | numeric | 12/0 |\n",
    "| V11 | F03\\[C-N\\]: Frequency of C-N at topological distance 3 | numeric | 21/0 |\n",
    "| V12 | SdssC: Sum of dssC E-states | numeric | 384/0 |\n",
    "| V13 | HyWi_B(m): Hyper-Wiener-like index (log function) from Burden matrix weighted by mass | numeric | 756/0 |\n",
    "| V14 | LOC: Lopping centric index | numeric | 373/0 |\n",
    "| V15 | SM6_L: Spectral moment of order 6 from Laplace matrix | numeric | 510/0 |\n",
    "| V16 | F03\\[C-O\\]: Frequency of C - O at topological distance 3 | numeric | 24/0 |\n",
    "| V17 | Me: Mean atomic Sanderson electronegativity (scaled on Carbon atom) | numeric | 167/0 |\n",
    "| V18 | Mi: Mean first ionization potential (scaled on Carbon atom) | numeric | 125/0 |\n",
    "| V19 | nN-N: Number of N hydrazines | numeric | 3/0 |\n",
    "| V20 | nArNO2: Number of nitro groups (aromatic) | numeric | 4/0 |\n",
    "| V21 | nCRX3: Number of CRX3 | numeric | 4/0 |\n",
    "| V22 | SpPosA_B(p): Normalized spectral positive sum from Burden matrix weighted by polarizability | numeric | 352/0 |\n",
    "| V23 | nCIR: Number of circuits | numeric | 13/0 |\n",
    "| V24 | B01\\[C-Br\\]: Presence/absence of C - Br at topological distance 1 | numeric | 2/0 |\n",
    "| V25 | B03\\[C-Cl\\]: Presence/absence of C - Cl at topological distance 3 | numeric | 2/0 |\n",
    "| V26 | N-073: Ar2NH / Ar3N / Ar2N- |  numeric   | 4/0    |\n",
    "| V27 | SpMax_A: Leading eigenvalue from adjacency matrix (Lovasz-Pelikan index) | numeric | 329/0 |\n",
    "| V28 | Psi\\_i\\_1d: Intrinsic state pseudoconnectivity index - type 1d | numeric | 205/0 |\n",
    "| V29 | B04\\[C-Br\\]: Presence/absence of C - Br at topological distance 4 | numeric | 2/0 |\n",
    "| V30 | SdO: Sum of dO E-states | numeric | 470/0 |\n",
    "| V31 | TI2_L: Second Mohar index from Laplace matrix | numeric | 553/0 |\n",
    "| V32 | nCrt: Number of ring tertiary C(sp3) | numeric | 8/0 |\n",
    "| V33 | C-026: R--CX--R | numeric | 11/0 |\n",
    "| V34 | F02\\[C-N\\]: Frequency of C - N at topological distance 2 | numeric | 16/0 |\n",
    "| V35 | nHDon: Number of donor atoms for H-bonds (N and O) | numeric | 8/0 |\n",
    "| V36 | SpMax_B(m): Leading eigenvalue from Burden matrix weighted by mass | numeric | 705/0 |\n",
    "| V37 | Psi\\_i\\_A: Intrinsic state pseudoconnectivity index - type S average |  numeric   | 624/0    |\n",
    "| V38 | nN: Number of Nitrogen atoms | numeric | 8/0 |\n",
    "| V39 | SM6_B(m): Spectral moment of order 6 from Burden matrix weighted by mass | numeric | 862/0 |\n",
    "| V40 | nArCOOR: Number of esters (aromatic) | numeric | 5/0 |\n",
    "| V41 | nX: Number of halogen atoms | numeric | 17/0 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataset. How balanced is the target variable (degradability)?\n",
    "dataset_train = pd.read_csv(\"train.csv\")\n",
    "dataset_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X_train = dataset_train.iloc[:, :-1].values\n",
    "y_train = dataset_train.iloc[:, -1].values\n",
    "\n",
    "X_test = dataset_test.iloc[:, :-1].values\n",
    "y_test = dataset_test.iloc[:, -1].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in X: 82\n",
      "Number of missing values in y: 0\n",
      "Sample 689 has more than one missing value\n",
      "Total number of samples with missing values: 81\n",
      "Number of samples in class 1: 564\n",
      "Number of samples in class 2: 282\n",
      "Indices of samples with missing values: [10, 13, 32, 42, 55, 60, 64, 66, 68, 69, 73, 78, 87, 89, 98, 104, 130, 131, 146, 152, 158, 171, 179, 188, 189, 223, 242, 246, 260, 268, 292, 302, 313, 326, 338, 360, 361, 373, 375, 390, 403, 410, 411, 431, 432, 439, 470, 498, 509, 511, 528, 544, 546, 556, 573, 577, 585, 589, 593, 595, 602, 612, 617, 648, 649, 665, 676, 678, 680, 689, 706, 719, 748, 752, 764, 766, 767, 791, 806, 809, 839]\n"
     ]
    }
   ],
   "source": [
    "num_missing = np.sum(np.isnan(X_train))\n",
    "print(f\"Number of missing values in X: {num_missing}\")\n",
    "num_missing = np.sum(np.isnan(y_train))\n",
    "print(f\"Number of missing values in y: {num_missing}\")\n",
    "\n",
    "# Check if there are any missing values present in the dataset\n",
    "# If there are, print the index of the sample\n",
    "total_missing = 0\n",
    "missing_indices = []\n",
    "for i in range(len(X_train)):\n",
    "    if np.isnan(X_train[i]).any():\n",
    "        missing_indices.append(i)\n",
    "        # Also check if line contains more than one missing value\n",
    "        if len(np.where(np.isnan(X_train[i]))[0]) > 1:\n",
    "            print(\"Sample {} has more than one missing value\".format(i))\n",
    "        total_missing += 1\n",
    "print(\"Total number of samples with missing values: {}\".format(total_missing))\n",
    "\n",
    "# Check how balanced is the target variable\n",
    "print(\"Number of samples in class 1: {}\".format(np.sum(y_train == 1)))\n",
    "print(\"Number of samples in class 2: {}\".format(np.sum(y_train == 2)))\n",
    "print(\"Indices of samples with missing values: {}\".format(sorted(missing_indices)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Taking care of the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with the mean of the column\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "imputer.fit(X_train)\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Encoding dependent variable ([1,2] -> [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "y_train = labelencoder.fit_transform(y_train)\n",
    "y_test = labelencoder.transform(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Feature Scaling\n",
    "\n",
    "Use `standardization`: $X = \\frac{X - \\mu}{\\sigma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "classifiers = dict()\n",
    "classifiers_tuned = dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg_classifier = LogisticRegression(random_state=0)\n",
    "lreg_classifier.fit(X_train, y_train)\n",
    "classifiers[\"Logistic Regression\"] = lreg_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparamter grid for logistic regression\n",
    "parameters = {\"C\": [0.1, 1, 10, 100, 1000], \"penalty\": [\"l1\", \"l2\"], \"solver\": [\"liblinear\"]}\n",
    "# Create the grid search object\n",
    "lreg_tuned = GridSearchCV(\n",
    "    estimator=lreg_classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "# Fit the grid search object to the training data\n",
    "lreg_tuned.fit(X_train, y_train)\n",
    "classifiers_tuned[\"Logistic Regression\"] = lreg_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "classifiers[\"K-Nearest Neighbors\"] = knn_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"n_neighbors\": [3, 5, 7, 9, 11, 13, 15, 17, 19, 21], \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]}\n",
    "knn_tuned = GridSearchCV(\n",
    "    estimator=knn_classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "knn_tuned.fit(X_train, y_train)\n",
    "classifiers_tuned[\"K-Nearest Neighbors\"] = knn_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Kernel Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier = SVC(kernel=\"rbf\", random_state=0)\n",
    "svc_classifier.fit(X_train, y_train)\n",
    "classifiers[\"Kernel Support Vector Classification\"] = svc_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"C\": [0.25, 0.5, 0.75, 1], \"kernel\": [\"linear\"]},\n",
    "    {\n",
    "        \"C\": [0.25, 0.5, 0.75, 1],\n",
    "        \"kernel\": [\"rbf\"],\n",
    "        \"gamma\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    },\n",
    "]\n",
    "svc_classifier_tuned = GridSearchCV(\n",
    "    estimator=svc_classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "svc_classifier_tuned.fit(X_train, y_train)\n",
    "classifiers_tuned[\"Kernel Support Vector Classification\"] = svc_classifier_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_classifier = GaussianNB()\n",
    "gauss_classifier.fit(X_train, y_train)\n",
    "classifiers[\"Naive Bayes\"] = gauss_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_classifier = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\n",
    "dtree_classifier.fit(X_train, y_train)\n",
    "classifiers[\"Decision Tree Classification\"] = dtree_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [3, 4, 5, 6, 7, 8, 9, 10, 12, 15, 20]}\n",
    "dtree_classifier_tuned = GridSearchCV(\n",
    "    estimator=dtree_classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "dtree_classifier_tuned.fit(X_train, y_train)\n",
    "classifiers_tuned[\"Decision Tree Classification\"] = dtree_classifier_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "rfc_classifier.fit(X_train, y_train)\n",
    "classifiers [\"Random Forest Classification\"] = rfc_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\": [10, 50, 100, 200, 300, 400, 500],\n",
    "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "    \"max_depth\": [None, 5, 10, 15, 20],\n",
    "}\n",
    "rfc_classifier_tuned = GridSearchCV(\n",
    "    estimator=rfc_classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rfc_classifier_tuned.fit(X_train, y_train)\n",
    "classifiers_tuned[\"Random Forest Classification\"] = rfc_classifier_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "classifiers[\"XGBoost\"] = xgb_classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.5, 1],\n",
    "    \"max_depth\": [1, 3, 6],\n",
    "    \"reg_lambda\": [0, 0.5, 1],\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500],\n",
    "}\n",
    "xgb_classifier_tuned = GridSearchCV(\n",
    "    estimator=xgb_classifier,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=10,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "xgb_classifier_tuned.fit(X_train, y_train)\n",
    "classifiers_tuned[\"XGBoost\"] = xgb_classifier_tuned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Logistic Regression------------------------------\n",
      "Accuracy of Logistic Regression is 86.40 %\n",
      "Standard Deviation of Logistic Regression is 4.29 %\n",
      "------------------------------K-Nearest Neighbors------------------------------\n",
      "Accuracy of K-Nearest Neighbors is 85.09 %\n",
      "Standard Deviation of K-Nearest Neighbors is 5.19 %\n",
      "------------------------------Kernel Support Vector Classification------------------------------\n",
      "Accuracy of Kernel Support Vector Classification is 86.16 %\n",
      "Standard Deviation of Kernel Support Vector Classification is 4.18 %\n",
      "------------------------------Naive Bayes------------------------------\n",
      "Accuracy of Naive Bayes is 67.36 %\n",
      "Standard Deviation of Naive Bayes is 5.32 %\n",
      "------------------------------Decision Tree Classification------------------------------\n",
      "Accuracy of Decision Tree Classification is 80.00 %\n",
      "Standard Deviation of Decision Tree Classification is 7.19 %\n",
      "------------------------------Random Forest Classification------------------------------\n",
      "Accuracy of Random Forest Classification is 84.86 %\n",
      "Standard Deviation of Random Forest Classification is 5.93 %\n",
      "------------------------------XGBoost------------------------------\n",
      "Accuracy of XGBoost is 84.15 %\n",
      "Standard Deviation of XGBoost is 5.05 %\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "    # Peroform k-fold cross validation\n",
    "    accuracies = cross_val_score(estimator=classifiers[classifier], X=X_train, y=y_train, cv=10)\n",
    "    print(\"-\"*30 + classifier + \"-\"*30)\n",
    "    print(\"Accuracy of {} is {:.2f} %\".format(classifier, accuracies.mean()*100))\n",
    "    print(\"Standard Deviation of {} is {:.2f} %\".format(classifier, accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------Logistic Regression------------------------------\n",
      "Best Accuracy of Logistic Regression is 86.28 %\n",
      "Best Parameters of Logistic Regression is {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Standard Deviation of Logistic Regression is 4.33 %\n",
      "------------------------------K-Nearest Neighbors------------------------------\n",
      "Best Accuracy of K-Nearest Neighbors is 85.32 %\n",
      "Best Parameters of K-Nearest Neighbors is {'metric': 'manhattan', 'n_neighbors': 17}\n",
      "Standard Deviation of K-Nearest Neighbors is 7.28 %\n",
      "------------------------------Kernel Support Vector Classification------------------------------\n",
      "Best Accuracy of Kernel Support Vector Classification is 86.64 %\n",
      "Best Parameters of Kernel Support Vector Classification is {'C': 0.25, 'kernel': 'linear'}\n",
      "Standard Deviation of Kernel Support Vector Classification is 3.89 %\n",
      "------------------------------Decision Tree Classification------------------------------\n",
      "Best Accuracy of Decision Tree Classification is 81.55 %\n",
      "Best Parameters of Decision Tree Classification is {'criterion': 'gini', 'max_depth': 6}\n",
      "Standard Deviation of Decision Tree Classification is 5.72 %\n",
      "------------------------------Random Forest Classification------------------------------\n",
      "Best Accuracy of Random Forest Classification is 86.28 %\n",
      "Best Parameters of Random Forest Classification is {'criterion': 'gini', 'max_depth': 15, 'n_estimators': 300}\n",
      "Standard Deviation of Random Forest Classification is 5.36 %\n",
      "------------------------------XGBoost------------------------------\n",
      "Best Accuracy of XGBoost is 86.04 %\n",
      "Best Parameters of XGBoost is {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300, 'reg_lambda': 0}\n",
      "Standard Deviation of XGBoost is 4.94 %\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers_tuned:\n",
    "    print(\"-\"*30 + classifier + \"-\"*30)\n",
    "    print(\"Best Accuracy of {} is {:.2f} %\".format(classifier, classifiers_tuned[classifier].best_score_*100))\n",
    "    print(\"Best Parameters of {} is {}\".format(classifier, classifiers_tuned[classifier].best_params_))\n",
    "    # print(\"Best Estimator of {} is {}\".format(classifier, classifiers_tuned[classifier].best_estimator_))\n",
    "    print(\"Standard Deviation of {} is {:.2f} %\".format(classifier, classifiers_tuned[classifier].cv_results_[\"std_test_score\"][classifiers_tuned[classifier].best_index_]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "219284b42a864982ed2952f129d7a94d2845dabbbd34dbf2f569db19af2d38f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
